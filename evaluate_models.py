import pandas as pd
import numpy as np
from catboost import CatBoostRegressor
from tensorflow import keras
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
df = pd.read_csv("traffic_data.csv")

print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
print(f"‚úì –ö–æ–ª–æ–Ω–∫–∏: {df.columns.tolist()}")
print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö:")
print(df.describe())

if df.isnull().sum().sum() > 0:
    print("\n‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–ø—É—Å–∫–∏:")
    print(df.isnull().sum())
    df = df.dropna()
    print(f"‚úì –ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è: {len(df)} –∑–∞–ø–∏—Å–µ–π")


print("\nüîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

df_agg = df.groupby('step').agg({
    'veh_count': 'mean',
    'waiting_time': 'mean',
    'CO2': 'mean'
}).reset_index()


df_agg['veh_waiting_ratio'] = df_agg['veh_count'] / (df_agg['waiting_time'] + 1)
df_agg['CO2_per_vehicle'] = df_agg['CO2'] / (df_agg['veh_count'] + 1)
df_agg['traffic_density'] = df_agg['veh_count'] * df_agg['waiting_time']

X = df_agg[["veh_count", "CO2", "veh_waiting_ratio", "CO2_per_vehicle", "traffic_density"]]

def calculate_optimal_green_time(row):
    """
    –≠–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –∑–µ–ª—ë–Ω–æ–≥–æ —Å–≤–µ—Ç–∞
    –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ —ç—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ–ª—É—á–µ–Ω—ã –∏–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
    """
    veh = row['veh_count']
    co2 = row['CO2']
    
    base_time = 20
    
    if veh < 3:
        traffic_factor = 0.5
    elif veh < 8:
        traffic_factor = 1.0
    elif veh < 15:
        traffic_factor = 1.5
    else:
        traffic_factor = 2.0
    
    if co2 > 1500:
        eco_factor = 1.2 
    else:
        eco_factor = 1.0
    
    optimal_time = base_time * traffic_factor * eco_factor
    return np.clip(optimal_time, 5, 90)

y = df_agg.apply(calculate_optimal_green_time, axis=1)

print(f"–ü—Ä–∏–∑–Ω–∞–∫–∏: {X.columns.tolist()}")
print(f"–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –∑–µ–ª—ë–Ω–æ–≥–æ (5-90 —Å–µ–∫)")
print(f"–°—Ä–µ–¥–Ω–µ–µ: {y.mean():.1f}s, –ú–µ–¥–∏–∞–Ω–∞: {y.median():.1f}s")

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)

print(f"\nüìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ:")
print(f"   Train: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)")
print(f"   Test:  {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)")

print("\nü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")

print("   ‚Üí CatBoost (—Å early stopping)...")
X_train_cb, X_val_cb, y_train_cb, y_val_cb = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42
)

cat_model = CatBoostRegressor(
    iterations=500,
    learning_rate=0.05,
    depth=6,
    loss_function='MAE',
    early_stopping_rounds=50,
    verbose=False,
    random_state=42
)
cat_model.fit(
    X_train_cb, y_train_cb,
    eval_set=(X_val_cb, y_val_cb),
    verbose=False
)

print("   ‚Üí Neural Network (—Å early stopping)...")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

nn_model = keras.Sequential([
    keras.layers.Dense(16, activation='relu', input_shape=(X.shape[1],)),
    keras.layers.BatchNormalization(),
    keras.layers.Dropout(0.1),
    keras.layers.Dense(8, activation='relu'),
    keras.layers.Dense(1)
])

nn_model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss='mae',
    metrics=['mae', 'mse']
)

early_stop = keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=15,
    restore_best_weights=True
)

history = nn_model.fit(
    X_train_scaled, y_train,
    epochs=200,
    batch_size=32,
    validation_split=0.2,
    callbacks=[early_stop],
    verbose=0
)

print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
print(f"   CatBoost: {cat_model.tree_count_} –¥–µ—Ä–µ–≤—å–µ–≤")
print(f"   NN: –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {len(history.history['loss'])}")

print("\nüîÆ –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")

y_pred_cat = cat_model.predict(X_test)
y_pred_nn = nn_model.predict(X_test_scaled, verbose=0).flatten()
y_pred_ensemble = (y_pred_cat + y_pred_nn) / 2

y_pred_cat = np.clip(y_pred_cat, 5, 90)
y_pred_nn = np.clip(y_pred_nn, 5, 90)
y_pred_ensemble = np.clip(y_pred_ensemble, 5, 90)

print("\n" + "="*70)
print("üìà –ú–ï–¢–†–ò–ö–ò –ö–ê–ß–ï–°–¢–í–ê")
print("="*70)

models = {
    'CatBoost': y_pred_cat,
    'Neural Network': y_pred_nn,
    'Ensemble': y_pred_ensemble
}

best_mae = float('inf')
best_model = None

for model_name, y_pred in models.items():
    print(f"\nü§ñ {model_name}:")
    print("-" * 70)
    
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    mape = np.mean(np.abs((y_test - y_pred) / (y_test + 1e-6))) * 100
    within_5s = np.mean(np.abs(y_test - y_pred) < 5) * 100
    within_10s = np.mean(np.abs(y_test - y_pred) < 10) * 100
    
    print(f"   MAE:           {mae:.2f}s  (—Å—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞)")
    print(f"   RMSE:          {rmse:.2f}s  (—à—Ç—Ä–∞—Ñ –∑–∞ –±–æ–ª—å—à–∏–µ –æ—à–∏–±–∫–∏)")
    print(f"   R¬≤:            {r2:.4f}  (–∫–∞—á–µ—Å—Ç–≤–æ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è: {r2*100:.1f}%)")
    print(f"   MAPE:          {mape:.2f}%")
    print(f"   –¢–æ—á–Ω–æ—Å—Ç—å ¬±5s:  {within_5s:.1f}%")
    print(f"   –¢–æ—á–Ω–æ—Å—Ç—å ¬±10s: {within_10s:.1f}%")
    
    if mae < best_mae:
        best_mae = mae
        best_model = model_name


print("\n" + "="*70)
print("üîç –í–ê–ñ–ù–û–°–¢–¨ –ü–†–ò–ó–ù–ê–ö–û–í (CatBoost)")
print("="*70)

feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': cat_model.feature_importances_
}).sort_values('importance', ascending=False)

for _, row in feature_importance.iterrows():
    print(f"   {row['feature']:20s}: {row['importance']:6.2f}")

print("\nüìä –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤...")

fig, axes = plt.subplots(2, 3, figsize=(18, 10))
fig.suptitle('–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π —Å–≤–µ—Ç–æ—Ñ–æ—Ä–∞', fontsize=16, fontweight='bold')

ax = axes[0, 0]
ax.scatter(y_test, y_pred_cat, alpha=0.6, s=30, edgecolors='k', linewidths=0.5)
ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
ax.set_xlabel('–†–µ–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è (—Å–µ–∫)', fontsize=10)
ax.set_ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è (—Å–µ–∫)', fontsize=10)
ax.set_title(f'CatBoost (MAE={mean_absolute_error(y_test, y_pred_cat):.2f}s)', fontsize=11)
ax.grid(True, alpha=0.3)

ax = axes[0, 1]
ax.scatter(y_test, y_pred_nn, alpha=0.6, s=30, color='green', edgecolors='k', linewidths=0.5)
ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
ax.set_xlabel('–†–µ–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è (—Å–µ–∫)', fontsize=10)
ax.set_ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è (—Å–µ–∫)', fontsize=10)
ax.set_title(f'Neural Network (MAE={mean_absolute_error(y_test, y_pred_nn):.2f}s)', fontsize=11)
ax.grid(True, alpha=0.3)

ax = axes[0, 2]
ax.scatter(y_test, y_pred_ensemble, alpha=0.6, s=30, color='purple', edgecolors='k', linewidths=0.5)
ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
ax.set_xlabel('–†–µ–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è (—Å–µ–∫)', fontsize=10)
ax.set_ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è (—Å–µ–∫)', fontsize=10)
ax.set_title(f'Ensemble (MAE={mean_absolute_error(y_test, y_pred_ensemble):.2f}s)', fontsize=11)
ax.grid(True, alpha=0.3)

ax = axes[1, 0]
errors_cat = y_test.values - y_pred_cat
errors_nn = y_test.values - y_pred_nn
ax.hist(errors_cat, bins=30, alpha=0.6, label='CatBoost', color='blue')
ax.hist(errors_nn, bins=30, alpha=0.6, label='Neural Net', color='green')
ax.axvline(x=0, color='red', linestyle='--', linewidth=2)
ax.set_xlabel('–û—à–∏–±–∫–∞ (—Å–µ–∫)', fontsize=10)
ax.set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞', fontsize=10)
ax.set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫', fontsize=11)
ax.legend()
ax.grid(True, alpha=0.3)

ax = axes[1, 1]
feature_importance.plot(kind='barh', x='feature', y='importance', ax=ax, legend=False, color='teal')
ax.set_xlabel('–í–∞–∂–Ω–æ—Å—Ç—å', fontsize=10)
ax.set_ylabel('')
ax.set_title('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (CatBoost)', fontsize=11)
ax.grid(True, alpha=0.3, axis='x')

ax = axes[1, 2]
mae_values = [mean_absolute_error(y_test, p) for p in [y_pred_cat, y_pred_nn, y_pred_ensemble]]
r2_values = [r2_score(y_test, p) * 10 for p in [y_pred_cat, y_pred_nn, y_pred_ensemble]]

x = np.arange(3)
width = 0.35

bars1 = ax.bar(x - width/2, mae_values, width, label='MAE (—Å–µ–∫)', color='skyblue')
bars2 = ax.bar(x + width/2, r2_values, width, label='R¬≤ √ó 10', color='orange')

ax.set_xlabel('–ú–æ–¥–µ–ª–∏', fontsize=10)
ax.set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ', fontsize=10)
ax.set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π', fontsize=11)
ax.set_xticks(x)
ax.set_xticklabels(['CatBoost', 'NN', 'Ensemble'])
ax.legend()
ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('model_evaluation_fixed.png', dpi=150, bbox_inches='tight')
print("‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: model_evaluation_fixed.png")


print("\n" + "="*70)
print("üö¶ –¢–ï–°–¢ –ù–ê –†–ï–ê–õ–¨–ù–´–• –°–¶–ï–ù–ê–†–ò–Ø–•")
print("="*70)

test_scenarios = [
    {"name": "–ù–æ—á—å (–º–∞–ª—ã–π —Ç—Ä–∞—Ñ–∏–∫)", "veh_count": 2, "CO2": 100},
    {"name": "–£—Ç—Ä–æ (—Å—Ä–µ–¥–Ω–∏–π —Ç—Ä–∞—Ñ–∏–∫)", "veh_count": 8, "CO2": 500},
    {"name": "–ü—Ä–æ–±–∫–∞", "veh_count": 15, "CO2": 1200},
    {"name": "–ß–∞—Å –ø–∏–∫", "veh_count": 20, "CO2": 1800}
]

for scenario in test_scenarios:
    waiting_time_est = scenario['veh_count'] * 2 
    veh_waiting_ratio = scenario['veh_count'] / (waiting_time_est + 1)
    CO2_per_vehicle = scenario['CO2'] / (scenario['veh_count'] + 1)
    traffic_density = scenario['veh_count'] * waiting_time_est
    
    X_scenario = np.array([[
        scenario['veh_count'],
        scenario['CO2'],
        veh_waiting_ratio,
        CO2_per_vehicle,
        traffic_density
    ]])
    X_scenario_scaled = scaler.transform(X_scenario)
    
    pred_cat = np.clip(cat_model.predict(X_scenario)[0], 5, 90)
    pred_nn = np.clip(nn_model.predict(X_scenario_scaled, verbose=0)[0][0], 5, 90)
    pred_ensemble = (pred_cat + pred_nn) / 2
    
    print(f"\nüìç {scenario['name']}:")
    print(f"   –ú–∞—à–∏–Ω—ã: {scenario['veh_count']}, CO2: {scenario['CO2']}g")
    print(f"   ‚Üí CatBoost:    {pred_cat:.1f}s")
    print(f"   ‚Üí Neural Net:  {pred_nn:.1f}s")
    print(f"   ‚Üí –ê–Ω—Å–∞–º–±–ª—å:    {pred_ensemble:.1f}s ‚≠ê")


print("\n" + "="*70)
print("üéØ –ò–¢–û–ì–û–í–´–ô –û–¢–ß–Å–¢")
print("="*70)
print(f"‚úÖ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model} (MAE = {best_mae:.2f}s)")
print(f"üìä –û–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {'–•–æ—Ä–æ—à–µ–µ' if best_mae < 5 else '–¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è'}")
print(f"‚ö†Ô∏è  –í–ê–ñ–ù–û: –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è —Å–æ–∑–¥–∞–Ω–∞ —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏!")
print(f"   –î–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –Ω—É–∂–Ω—ã –¥–∞–Ω–Ω—ã–µ –∏–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å SUMO.")

results_df = pd.DataFrame({
    'y_test': y_test.values,
    'y_pred_catboost': y_pred_cat,
    'y_pred_nn': y_pred_nn,
    'y_pred_ensemble': y_pred_ensemble,
    'abs_error_catboost': np.abs(y_test.values - y_pred_cat),
    'abs_error_nn': np.abs(y_test.values - y_pred_nn),
    'abs_error_ensemble': np.abs(y_test.values - y_pred_ensemble)
})
results_df.to_csv('model_predictions_fixed.csv', index=False)
print("\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: model_predictions_fixed.csv")
print("="*70)